import dotenv from "dotenv";
dotenv.config();

console.log("ðŸ” Vision Controller Loaded (Vocabulary-based)");

// Predefined Kannada vocabulary
const OBJECT_VOCABULARY = {
  bottle: { kannada: "à²¬à²¾à²Ÿà²²à²¿", phonetic: "Batali", english: "Bottle" },
  book: { kannada: "à²ªà³à²¸à³à²¤à²•", phonetic: "Pustaka", english: "Book" },
  cup: { kannada: "à²•à²ªà³", phonetic: "Kap", english: "Cup" },
  phone: { kannada: "à²®à³Šà²¬à³ˆà²²à³", phonetic: "Mobile", english: "Phone" },
  mobile: { kannada: "à²®à³Šà²¬à³ˆà²²à³", phonetic: "Mobile", english: "Phone" },
  laptop: { kannada: "à²²à³à²¯à²¾à²ªà³à²Ÿà²¾à²ªà³", phonetic: "Laptop", english: "Laptop" },
  computer: { kannada: "à²²à³à²¯à²¾à²ªà³à²Ÿà²¾à²ªà³", phonetic: "Laptop", english: "Laptop" },
  pen: { kannada: "à²ªà³†à²¨à³", phonetic: "Pen", english: "Pen" },
  chair: { kannada: "à²•à³à²°à³à²šà²¿", phonetic: "Kurchi", english: "Chair" },
  table: { kannada: "à²®à³‡à²œà³", phonetic: "Meju", english: "Table" },
  bag: { kannada: "à²šà³€à²²", phonetic: "Chela", english: "Bag" },
  watch: { kannada: "à²—à²¡à²¿à²¯à²¾à²°", phonetic: "Gadiyara", english: "Watch" },
  key: { kannada: "à²•à³€à²²à²¿", phonetic: "Kili", english: "Key" },
  glass: { kannada: "à²—à³à²²à²¾à²¸à³", phonetic: "Glass", english: "Glass" },
  plate: { kannada: "à²¤à²Ÿà³à²Ÿà³†", phonetic: "Tatte", english: "Plate" },
  spoon: { kannada: "à²šà²®à²š", phonetic: "Chamacha", english: "Spoon" },
  knife: { kannada: "à²šà²¾à²•à³", phonetic: "Chaku", english: "Knife" },
  shoe: { kannada: "à²¶à³‚", phonetic: "Shoe", english: "Shoe" },
  hat: { kannada: "à²Ÿà³‹à²ªà²¿", phonetic: "Topi", english: "Hat" },
  clock: { kannada: "à²—à²¡à²¿à²¯à²¾à²°", phonetic: "Gadiyara", english: "Clock" },
  mirror: { kannada: "à²•à²¨à³à²¨à²¡à²¿", phonetic: "Kannadi", english: "Mirror" },
  door: { kannada: "à²¬à²¾à²—à²¿à²²à³", phonetic: "Bagilu", english: "Door" },
};

// Simulated object detection (for demo purposes)
// In production, you would use a real computer vision API
function detectObjectFromImage(imageBase64) {
  // This is a simple demo implementation
  // For a real app, you'd use:
  // - Google Cloud Vision API
  // - AWS Rekognition
  // - Azure Computer Vision
  // - TensorFlow.js

  // For now, return a random object for demo
  const objects = Object.keys(OBJECT_VOCABULARY);
  const randomObject = objects[Math.floor(Math.random() * objects.length)];

  console.log(`ðŸŽ² Demo mode: Randomly selected "${randomObject}"`);
  return randomObject;
}

export const analyzeImage = async (req, res) => {
  try {
    console.log("ðŸ“¸ Received image analysis request");

    const { imageBase64 } = req.body;

    if (!imageBase64) {
      console.log("âŒ No image provided");
      return res.status(400).json({
        success: false,
        message: "Image is required",
      });
    }

    console.log("âœ… Image received, analyzing...");

    // Simulate object detection
    // NOTE: This is a DEMO implementation
    // Replace with real computer vision API in production
    const detectedObject = detectObjectFromImage(imageBase64);

    console.log("ðŸŽ¯ Detected object:", detectedObject);

    // Check if object is in vocabulary
    if (OBJECT_VOCABULARY[detectedObject]) {
      const translation = OBJECT_VOCABULARY[detectedObject];
      console.log("âœ… Object found in vocabulary:", translation);

      return res.json({
        success: true,
        detected: detectedObject,
        english: translation.english,
        kannada: translation.kannada,
        phonetic: translation.phonetic,
        note: "Demo mode: Random object selection. Integrate real vision API for production.",
      });
    } else {
      console.log("âŒ Object not in vocabulary");
      return res.json({
        success: false,
        message: "Object not recognized. Try: bottle, book, cup, phone, etc.",
      });
    }
  } catch (error) {
    console.error("âŒ Analysis Error:", error);

    res.status(500).json({
      success: false,
      message: "Failed to analyze image",
      error: error.message,
    });
  }
};

// Get list of supported objects
export const getSupportedObjects = async (req, res) => {
  try {
    const objects = Object.values(OBJECT_VOCABULARY).map((obj) => ({
      english: obj.english,
      kannada: obj.kannada,
      phonetic: obj.phonetic,
    }));

    res.json({ objects });
  } catch (error) {
    console.error("Error fetching objects:", error);
    res.status(500).json({ message: "Failed to fetch objects" });
  }
};
